{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8df954d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/teo/userdata/virtuosoNet\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4044b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc14335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtuoso.train import prepare_dataloader\n",
    "from virtuoso.parser import get_parser\n",
    "from virtuoso import utils, model as modelzoo\n",
    "from virtuoso.dataset import ScorePerformDataset, FeatureCollate\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b99dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args = parser.parse_args(\n",
    "    args=[\"--yml_path=ymls/han_measnote.yml\",\n",
    "          \"--data_path=datasets/main_dataset_clamped\",\n",
    "          \"--emotion_data_path=datasets/emotion_dataset_clamped\",\n",
    "          \"--device=cpu\"]\n",
    ")\n",
    "args, net_params, configs = utils.handle_args(args)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3f6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../virtuosonet_checkpoints/yml_path=ymls/han_measnote.yml data_path=main_dataset_clamped emotion_data_path=emotion_dataset_clamped meas_note=True iters_per_checkpoint=300 delta_weight=5.0 delta_loss=True vel_balance_loss=True intermediate_loss=False graph_keys=[]_220112-153119/checkpoint_last.pt'\n",
    "# checkpoint_path = '../virtuosonet_checkpoints/yml_path=ymls/isgn_beatmeas_x.yml meas_note=True batch_size=24 iters_per_checkpoint=300 len_valid_slice=1200 delta_weight=5.0 delta_loss=True vel_balance_loss=True_220118-210647/checkpoint_last.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251413d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "args.yml_path = next(Path(checkpoint_path).parent.glob('*.yml'))\n",
    "net_param = torch.load(checkpoint_path, map_location='cpu')['network_params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa6bd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.graph_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea1fa7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelzoo.make_model(net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce9e57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint '../virtuosonet_checkpoints/yml_path=ymls/han_measnote.yml data_path=main_dataset_clamped emotion_data_path=emotion_dataset_clamped meas_note=True iters_per_checkpoint=300 delta_weight=5.0 delta_loss=True vel_balance_loss=True intermediate_loss=False graph_keys=[]_220112-153119/checkpoint_last.pt' (epoch 77)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VirtuosoNet(\n",
       "  (score_encoder): HanEncoder(\n",
       "    (note_fc): Sequential(\n",
       "      (0): Linear(in_features=79, out_features=128, bias=True)\n",
       "    )\n",
       "    (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (voice_net): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (beat_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (beat_rnn): LSTM(512, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (measure_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (measure_rnn): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (performance_encoder): HanPerfEncoder(\n",
       "    (performance_measure_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (performance_encoder): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (performance_final_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (performance_encoder_mean): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (performance_encoder_var): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (performance_note_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
       "    (performance_embedding_layer): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (performance_contractor): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=64, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (residual_info_selector): TempoVecMeasSelector()\n",
       "  (performance_decoder): HanMeasNoteDecoder(\n",
       "    (result_for_tempo_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=10, out_features=10, bias=True)\n",
       "    )\n",
       "    (beat_tempo_fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (style_vector_expandor): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    )\n",
       "    (measure_out_lstm): LSTM(330, 128, batch_first=True)\n",
       "    (measure_out_fc): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (beat_tempo_forward): LSTM(589, 128, batch_first=True)\n",
       "    (output_lstm): LSTM(1101, 64, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = utils.load_weight(model, checkpoint_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a40189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hier_type = ['is_hier', 'in_hier', 'hier_beat', 'hier_meas', 'meas_note']\n",
    "curr_type = [x for x in hier_type if getattr(args, x)]\n",
    "\n",
    "train_set = ScorePerformDataset(args.data_path, \n",
    "                                type=\"train\", \n",
    "                                len_slice=args.len_slice, \n",
    "                                len_graph_slice=args.len_graph_slice, \n",
    "                                graph_keys=args.graph_keys, \n",
    "                                hier_type=curr_type)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "small_train_loader = DataLoader(train_set, batch_size=5, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "tiny_train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2976b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_x, batch_y, beat_y, meas_y, note_locations, align_matched, pedal_status, edges \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbatch_to_device(batch, device)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 7\u001b[0m outputs, perform_mu, perform_var, total_out_list \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnote_locations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/userdata/virtuosoNet/virtuoso/model.py:59\u001b[0m, in \u001b[0;36mVirtuosoNet.forward\u001b[0;34m(self, x, y, edges, note_locations, initial_z)\u001b[0m\n\u001b[1;32m     57\u001b[0m score_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_encoder(x, edges, note_locations)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     performance_embedding, perform_mu, perform_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperformance_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnote_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(initial_z) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m initial_z \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/userdata/virtuosoNet/virtuoso/encoder_perf.py:61\u001b[0m, in \u001b[0;36mPerformanceEncoder.forward\u001b[0;34m(self, score_embedding, y, edges, note_locations, return_z, num_samples)\u001b[0m\n\u001b[1;32m     58\u001b[0m perform_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((total_note_cat, expanded_y), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     59\u001b[0m perform_concat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masking_notes(perform_concat)\n\u001b[0;32m---> 61\u001b[0m perform_z, perform_mu, perform_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_perform_style_from_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperform_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_numbers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_z:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_multiple_z(perform_mu, perform_var, num_samples)\n",
      "File \u001b[0;32m~/userdata/virtuosoNet/virtuoso/encoder_perf.py:30\u001b[0m, in \u001b[0;36mPerformanceEncoder._get_perform_style_from_input\u001b[0;34m(self, perform_concat, edges, measure_numbers)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_perform_style_from_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, perform_concat, edges, measure_numbers):\n\u001b[1;32m     29\u001b[0m   perform_style_contracted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperformance_contractor(perform_concat)\n\u001b[0;32m---> 30\u001b[0m   perform_style_graphed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_note_hidden_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperform_style_contracted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m   performance_measure_nodes \u001b[38;5;241m=\u001b[39m make_higher_node(perform_style_graphed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperformance_measure_attention, measure_numbers,\n\u001b[1;32m     32\u001b[0m                                           measure_numbers, lower_is_note\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m   perform_style_encoded \u001b[38;5;241m=\u001b[39m run_hierarchy_lstm_with_pack(performance_measure_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperformance_encoder)\n",
      "File \u001b[0;32m~/userdata/virtuosoNet/virtuoso/encoder_perf.py:85\u001b[0m, in \u001b[0;36mHanPerfEncoder._get_note_hidden_states\u001b[0;34m(self, perform_style_contracted, edges)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_note_hidden_states\u001b[39m(\u001b[38;5;28mself\u001b[39m, perform_style_contracted, edges):\n\u001b[1;32m     84\u001b[0m   perform_note_encoded, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperformance_note_encoder(perform_style_contracted)\n\u001b[0;32m---> 85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m perform_note_encoded\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "train_set.update_slice_info()\n",
    "batch = next(iter(train_loader))\n",
    "batch_x, batch_y, beat_y, meas_y, note_locations, align_matched, pedal_status, edges = utils.batch_to_device(batch, device)\n",
    "\n",
    "model.eval()\n",
    "outputs, perform_mu, perform_var, total_out_list = model(batch_x, batch_y, edges, note_locations)\n",
    "# with torch.no_grad():\n",
    "#   score_embedding = model.score_encoder(batch_x, edges, note_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e05ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import sys\n",
    "\n",
    "# sys.path.append('../')\n",
    "\n",
    "\n",
    "\n",
    "from virtuoso.train import prepare_dataloader\n",
    "from virtuoso.parser import get_parser\n",
    "from virtuoso import utils, model as modelzoo\n",
    "from virtuoso.dataset import ScorePerformDataset, FeatureCollate\n",
    "import random\n",
    "import torch\n",
    "\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(\n",
    "    args=[\"--yml_path=ymls/han_measnote.yml\",\n",
    "          \"--data_path=datasets/main_dataset_clamped\",\n",
    "          \"--emotion_data_path=datasets/emotion_dataset_clamped\",\n",
    "          \"--device=cpu\"]\n",
    ")\n",
    "args, net_params, configs = utils.handle_args(args)\n",
    "device = 'cpu'\n",
    "\n",
    "checkpoint_path = '../virtuosonet_checkpoints/yml_path=ymls/han_measnote.yml data_path=main_dataset_clamped emotion_data_path=emotion_dataset_clamped meas_note=True iters_per_checkpoint=300 delta_weight=5.0 delta_loss=True vel_balance_loss=True intermediate_loss=False graph_keys=[]_220112-153119/checkpoint_last.pt'\n",
    "# checkpoint_path = '../virtuosonet_checkpoints/yml_path=ymls/isgn_beatmeas_x.yml meas_note=True batch_size=24 iters_per_checkpoint=300 len_valid_slice=1200 delta_weight=5.0 delta_loss=True vel_balance_loss=True_220118-210647/checkpoint_last.pt'\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "args.yml_path = next(Path(checkpoint_path).parent.glob('*.yml'))\n",
    "net_param = torch.load(checkpoint_path, map_location='cpu')['network_params']\n",
    "\n",
    "\n",
    "args.graph_keys\n",
    "\n",
    "model = modelzoo.make_model(net_params)\n",
    "\n",
    "model = utils.load_weight(model, checkpoint_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model.network_params.num_edge_types\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hier_type = ['is_hier', 'in_hier', 'hier_beat', 'hier_meas', 'meas_note']\n",
    "curr_type = [x for x in hier_type if getattr(args, x)]\n",
    "\n",
    "train_set = ScorePerformDataset(args.data_path, \n",
    "                                type=\"train\", \n",
    "                                len_slice=args.len_slice, \n",
    "                                len_graph_slice=args.len_graph_slice, \n",
    "                                graph_keys=args.graph_keys, \n",
    "                                hier_type=curr_type)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "small_train_loader = DataLoader(train_set, batch_size=5, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "tiny_train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "train_set.update_slice_info()\n",
    "batch = next(iter(train_loader))\n",
    "batch_x, batch_y, beat_y, meas_y, note_locations, align_matched, pedal_status, edges = utils.batch_to_device(batch, device)\n",
    "\n",
    "# model.eval()\n",
    "# # outputs, perform_mu, perform_var, total_out_list = model(batch_x, batch_y, edges, note_locations)\n",
    "# with torch.no_grad():\n",
    "#   score_embedding = model.score_encoder(batch_x, edges, note_locations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
