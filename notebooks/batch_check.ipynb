{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4044b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc14335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtuoso.train import prepare_dataloader\n",
    "from virtuoso.parser import get_parser\n",
    "from virtuoso import utils, model as modelzoo\n",
    "from virtuoso.dataset import ScorePerformDataset, FeatureCollate\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b99dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args = parser.parse_args(\n",
    "    args=[\"--yml_path=../ymls/han_measnote.yml\",\n",
    "          \"--data_path=../dataset_section_tempo\",\n",
    "          \"--emotion_data_path=../dataset_emotion_section_tempo\",\n",
    "          \"--device=cpu\"]\n",
    ")\n",
    "args, net_params, configs = utils.handle_args(args)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea1fa7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelzoo.make_model(net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ce9e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint '../../virtuosonet_checkpoints/yml_path=ymls/han_measnote.yml iters_per_checkpoint=300 delta_weight=10.0 delta_loss=True vel_balance_loss=True intermediate_loss=False_220108-152314/checkpoint_last.pt' (epoch 99)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VirtuosoNet(\n",
       "  (score_encoder): HanEncoder(\n",
       "    (note_fc): Sequential(\n",
       "      (0): Linear(in_features=78, out_features=128, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (voice_net): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (beat_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (beat_rnn): LSTM(512, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (measure_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (measure_rnn): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (performance_encoder): HanPerfEncoder(\n",
       "    (performance_note_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
       "    (performance_measure_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (performance_embedding_layer): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (performance_contractor): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=64, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (performance_encoder): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (performance_final_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (performance_encoder_mean): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (performance_encoder_var): Linear(in_features=128, out_features=16, bias=True)\n",
       "  )\n",
       "  (residual_info_selector): TempoVecMeasSelector()\n",
       "  (performance_decoder): HanMeasNoteDecoder(\n",
       "    (result_for_tempo_attention): ContextAttention(\n",
       "      (attention_net): Linear(in_features=10, out_features=10, bias=True)\n",
       "    )\n",
       "    (beat_tempo_fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (style_vector_expandor): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (measure_out_lstm): LSTM(330, 128, batch_first=True)\n",
       "    (measure_out_fc): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (beat_tempo_forward): LSTM(589, 128, batch_first=True)\n",
       "    (output_lstm): LSTM(1101, 64, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = '../../virtuosonet_checkpoints/yml_path=ymls/han_measnote.yml iters_per_checkpoint=300 delta_weight=10.0 delta_loss=True vel_balance_loss=True intermediate_loss=False_220108-152314/checkpoint_last.pt'\n",
    "model = utils.load_weight(model, checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65a40189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hier_type = ['is_hier', 'in_hier', 'hier_beat', 'hier_meas', 'meas_note']\n",
    "curr_type = [x for x in hier_type if getattr(args, x)]\n",
    "\n",
    "train_set = ScorePerformDataset(args.data_path, \n",
    "                                type=\"train\", \n",
    "                                len_slice=args.len_slice, \n",
    "                                len_graph_slice=args.len_graph_slice, \n",
    "                                graph_keys=args.graph_keys, \n",
    "                                hier_type=curr_type)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "small_train_loader = DataLoader(train_set, batch_size=5, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n",
    "tiny_train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, collate_fn=FeatureCollate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea2976b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_set.update_slice_info()\n",
    "batch = next(iter(train_loader))\n",
    "batch_x, batch_y, beat_y, meas_y, note_locations, align_matched, pedal_status, edges = utils.batch_to_device(batch, device)\n",
    "\n",
    "model.eval()\n",
    "# outputs, perform_mu, perform_var, total_out_list = model(batch_x, batch_y, edges, note_locations)\n",
    "score_embedding = model.score_encoder(batch_x, edges, note_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b93eb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 818, 78]), torch.Size([5, 818, 78]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape, small_batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10a52a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_set.update_slice_info()\n",
    "small_batch = next(iter(small_train_loader))\n",
    "small_batch_x, small_batch_y, beat_y, meas_y, small_note_locations, align_matched, pedal_status, edges = utils.batch_to_device(small_batch, device)\n",
    "# small_outputs, small_perform_mu, small_perform_var, total_out_list = model(small_batch_x, batch_y, edges, note_locations)\n",
    "model.eval()\n",
    "score_embedding_small = model.score_encoder(small_batch_x, edges, small_note_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dda276f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 818, 11])\n",
      "torch.Size([5, 818, 11])\n",
      "tensor([[[ 7.4506e-08, -8.9407e-08,  2.9802e-08,  ..., -1.4901e-08,\n",
      "           5.9605e-08,  1.1921e-07],\n",
      "         [ 7.4506e-08,  5.9605e-08,  7.4506e-08,  ..., -2.9802e-08,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 7.4506e-08, -1.7881e-07,  4.4703e-08,  ...,  5.9605e-08,\n",
      "           1.1921e-07,  1.1921e-07],\n",
      "         ...,\n",
      "         [ 2.9802e-08, -2.0862e-07, -1.0617e-07,  ...,  1.1921e-07,\n",
      "          -2.3842e-07,  1.1921e-07],\n",
      "         [ 2.9802e-08, -5.9605e-08, -6.5193e-08,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -1.1921e-07, -1.1874e-08,  ...,  5.9605e-08,\n",
      "           0.0000e+00,  1.1921e-07]],\n",
      "\n",
      "        [[-4.4703e-08,  0.0000e+00,  4.4703e-08,  ...,  0.0000e+00,\n",
      "           5.9605e-08,  0.0000e+00],\n",
      "         [-4.4703e-08, -8.9407e-08, -2.9802e-08,  ...,  5.9605e-08,\n",
      "           5.9605e-08,  5.9605e-08],\n",
      "         [-4.4703e-08,  5.9605e-08, -2.9802e-08,  ..., -5.9605e-08,\n",
      "           1.7881e-07,  5.9605e-08],\n",
      "         ...,\n",
      "         [ 7.4506e-08,  1.1921e-07, -5.2154e-08,  ..., -5.9605e-08,\n",
      "          -2.3842e-07,  2.3842e-07],\n",
      "         [ 7.4506e-08,  1.4901e-08,  2.2352e-08,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  1.1921e-07],\n",
      "         [ 7.4506e-08,  1.0431e-07,  5.2154e-08,  ..., -2.9802e-08,\n",
      "          -5.9605e-08,  0.0000e+00]],\n",
      "\n",
      "        [[-1.1921e-07,  5.9605e-08, -8.9407e-08,  ...,  1.4901e-07,\n",
      "           5.9605e-08, -1.4901e-07],\n",
      "         [-1.1921e-07,  3.2783e-07,  1.1921e-07,  ...,  5.9605e-08,\n",
      "           1.7881e-07,  0.0000e+00],\n",
      "         [-1.1921e-07, -2.9802e-08,  1.1921e-07,  ...,  0.0000e+00,\n",
      "           1.7881e-07,  5.9605e-08],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  8.9407e-08,  0.0000e+00,  ...,  5.9605e-08,\n",
      "           1.1921e-07,  2.3842e-07],\n",
      "         [ 0.0000e+00,  1.1921e-07,  5.9605e-08,  ..., -5.9605e-08,\n",
      "          -1.1921e-07,  5.9605e-08],\n",
      "         [-2.3842e-07, -1.7881e-07, -1.0431e-07,  ...,  1.1921e-07,\n",
      "           1.1921e-07,  0.0000e+00]],\n",
      "\n",
      "        [[-5.9605e-08,  1.1921e-07, -2.2352e-08,  ...,  2.9802e-08,\n",
      "           0.0000e+00, -4.4703e-08],\n",
      "         [-5.9605e-08, -1.1921e-07, -7.4506e-08,  ...,  1.1921e-07,\n",
      "           1.7881e-07, -2.9802e-08],\n",
      "         [-5.9605e-08, -1.1921e-07, -1.3411e-07,  ...,  5.9605e-08,\n",
      "           0.0000e+00,  1.4901e-07],\n",
      "         ...,\n",
      "         [-5.9605e-08,  1.7881e-07, -5.9605e-08,  ...,  5.9605e-08,\n",
      "          -1.1921e-07, -2.3842e-07],\n",
      "         [-5.9605e-08,  0.0000e+00,  1.4901e-08,  ...,  1.1921e-07,\n",
      "          -2.3842e-07, -1.7881e-07],\n",
      "         [-5.9605e-08,  2.9802e-07, -4.4703e-08,  ...,  1.7881e-07,\n",
      "           0.0000e+00, -1.7881e-07]]], grad_fn=<SubBackward0>)\n",
      "tensor(3.8743e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from virtuoso.model_utils import masking_half, make_higher_node, encode_with_net\n",
    "\n",
    "def compare_output(model, batch_a, batch_b, note_locations_a, note_locations_b, y_a, y_b, target_id=0):\n",
    "  torch.manual_seed(0)\n",
    "  out_a = desired_computation(model, batch_a, note_locations_a, y_a)\n",
    "  torch.manual_seed(0)\n",
    "  out_b = desired_computation(model, batch_b, note_locations_b, y_b)\n",
    "  return out_a[:4, :30] - out_b[:4, :30] \n",
    "\n",
    "def desired_computation(model, batch_x, note_locations, y):\n",
    "  score_embedding = model.score_encoder(batch_x, None, note_locations)\n",
    "  performance_embedding, perform_mu, perform_var = model.performance_encoder(score_embedding, y, edges, note_locations, return_z=False)\n",
    "  residual_info = model.residual_info_selector(batch_x, note_locations)\n",
    "  \n",
    "#   perform_z = model.performance_decoder.style_vector_expandor(performance_embedding)\n",
    "#   perform_z = model.performance_decoder.handle_style_vector(performance_embedding)\n",
    "#   _, measure_tempo_vel = model.performance_decoder.run_measure_level(score_embedding, perform_z, residual_info, note_locations)\n",
    "\n",
    "  output, alter_out = model.performance_decoder(score_embedding, performance_embedding, residual_info, edges, note_locations)\n",
    "  print(output.shape)\n",
    "  return output\n",
    "  \n",
    "model=model.eval()\n",
    "com_result = compare_output(model, batch_x, small_batch_x, note_locations, small_note_locations, batch_y, small_batch_y)\n",
    "print(com_result)\n",
    "print(torch.max(com_result))\n",
    "# print((torch.abs(com_result)<1e-6).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1e671302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score_encoder.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228c97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d335a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 818, 512]), torch.Size([4, 814, 512]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_embedding['note'].shape, score_embedding_small['note'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29a83cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00, -1.1176e-08,  ..., -7.4506e-09,\n",
       "         -1.1176e-08,  1.7695e-08],\n",
       "        [ 0.0000e+00, -1.4901e-08, -1.4901e-08,  ...,  2.9802e-08,\n",
       "         -1.4901e-08,  0.0000e+00],\n",
       "        [ 4.4703e-08,  1.1176e-08,  7.4506e-09,  ...,  1.4901e-08,\n",
       "          7.4506e-09, -1.8626e-08],\n",
       "        ...,\n",
       "        [-1.4901e-08,  2.9802e-08,  4.4703e-08,  ...,  1.4901e-08,\n",
       "          1.7229e-08, -1.4901e-08],\n",
       "        [-2.2352e-08, -5.9605e-08,  1.4901e-08,  ...,  1.4901e-08,\n",
       "          2.5146e-08,  2.9802e-08],\n",
       "        [ 1.6764e-08, -1.1921e-07,  2.9802e-08,  ...,  2.9802e-08,\n",
       "          1.3039e-08,  0.0000e+00]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_embedding['note'][0][:10] - score_embedding_small['note'][0][:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "459769eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((batch_x[0,:814] - small_batch_x[0]) == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94fc8011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0,:814] == small_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8e8300",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [818] at index 0 does not match the shape of the indexed tensor [755, 128] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2938602/3793131288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperform_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperform_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_out_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_batch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_note_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/userdata/virtuosoNet/notebooks/../virtuoso/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, edges, note_locations, initial_z)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mscore_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_z\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mperformance_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperform_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperform_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperformance_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/userdata/virtuosoNet/notebooks/../virtuoso/encoder_score.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edges, note_locations)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mmax_voice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mvoice_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_voice_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoice_numbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_voice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0mhidden_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# out: tensor of shape (batch_size, seq_length, hidden_size*2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mhidden_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/userdata/virtuosoNet/notebooks/../virtuoso/encoder_score.py\u001b[0m in \u001b[0;36mrun_voice_net\u001b[0;34m(self, batch_x, voice_numbers, max_voice)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mnum_voice_notes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0mvoice_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoice_x_bool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_x_bool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mvoice_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mpack_voice_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvoice_notes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/userdata/virtuosoNet/notebooks/../virtuoso/encoder_score.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mnum_voice_notes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0mvoice_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoice_x_bool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_x_bool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mvoice_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mpack_voice_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvoice_notes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [818] at index 0 does not match the shape of the indexed tensor [755, 128] at index 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cb473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
